{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "We often set out to learn and/or use a particular API or library in the context of having to get something done quickly to meet a business objective.  Being time crunched, we commonly fall into the mindset of treating things as ***black boxes***.  In other word, we just make sure that we give the black box the right input and checking that it gives us a reasonable output.\n",
    "\n",
    "<img src='00_black_box.png'>\n",
    "\n",
    "This hacking mentality works fine for most APIs, but applying this approach to **Spark** can quickly rob you of the performance improvements it can provide.  It is common to implement Spark in an analysis or application and not see any significant performance improvement.  This is often because the developer isn't using the API in the way it was designed to be used.\n",
    "\n",
    "\n",
    "The main focus of this presentation is to leave you with enough of an understanding of how **Spark** does what it does so that when you actually start using it, you'll have a good sense for how to use it in a way that gives you the significant performance gains that motivated you to use Spark in the first place.\n",
    "\n",
    "### Acknowledgement\n",
    "\n",
    "Most of the content of this presentation came from lecture materials from this class:\n",
    "\n",
    "https://www.edx.org/course/big-data-analytics-using-spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark - What problems does it try to solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two big problems with \"big\" data:\n",
    "\n",
    "### 1. Memory capacity\n",
    "\n",
    "How does data that's larger than the memory capacity of your local system get processes?\n",
    "\n",
    "+ buy more memory\n",
    "+ abstract memory to use hard drive space: virtual memory\n",
    "+ distributed computing with specialized hardware (the early and expensive solution)\n",
    "+ distributed computing with comodity hardware (the modern, cost-effective solution)\n",
    "  - This is what **Hadoop** and **Spark** enable us to do.\n",
    "\n",
    "### 2. Computational throughput\n",
    "\n",
    "Even after addressing the memory issue, what about speed?  Even with hardware capable of distibuted computing, making good use of this hardware to run computations effciently has typically required abandoning high-level languages like Java, Python, R etc. for more low-level languages that can control how the hardware does its job e.g. machine language.\n",
    "\n",
    "### What Spark brings to the table\n",
    "\n",
    "+ Spark provides APIs for high-level languages to implement parallel processing on large datasets.\n",
    "+ It's generally much faster than **Hadoop** which uses disks for storage and depends on disk read and write speeds\n",
    "+ Spark does much of its processing in memory and tries to minimize disk reads and writes.\n",
    "+ See the chart in the Appendix for a detailed comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power of Spark comes from minimizing latencies\n",
    "\n",
    "What are sources of latency?\n",
    "\n",
    "<img src=\"./01.02b Latencies and Storage Types_small.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Locality\n",
    "\n",
    "+ Access locality refers to the ability of software to make good use of the cache and memory\n",
    "  - cache = super fast, but small memory (L1 and L2) in CPU holding commands and data from prior processing requests\n",
    "+ Memory is broken up into pages.\n",
    "+ Software that uses the same or neighboring pages repeatedly has good access locality.\n",
    "+ Hardware is designed to speed up such software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two types of Locality\n",
    "\n",
    "<img src=\"01.03b slide - temporal locality_small.png\">\n",
    "\n",
    "Temporal locality means that you're using the same elements again and again in quick succession.  The degree of this reuse is the degree to which we have temporal locality.\n",
    "\n",
    "<img src=\"01.03c slide - spatial locality_small.png\">\n",
    "\n",
    "Spatial locality is minimizing the physical distance between elements used in a calculation.\n",
    "\n",
    "Linked lists (shown below) make good use of space, but have poor spatial locality.  Indexed arrays are typically layed out in consecutive memory locations which gives them good spatial locality.  A similar array to the one shown below would be allocated on two pages instead of 4.\n",
    "\n",
    "<img src=\"01.03d slide - linked list vs array_small.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./01.04b slide - basic idea behind cache_small.png\">\n",
    "\n",
    "### Cache Hits are fast\n",
    "\n",
    "If CPU needs something that is already in cache, it doesn't need to go off and grab it from storage:\n",
    "\n",
    "<img src=\"01.04c slide - cache hit_small.png\">\n",
    "\n",
    "### Cache Misses are slow\n",
    "\n",
    "Three steps in processing a cache miss\n",
    "\n",
    "1. choose byte in cache to drop\n",
    "2. delete the item in the cache\n",
    "3. read in the requested byte\n",
    "\n",
    "<img src=\"01.04e slide - cache miss1,2and3_small.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache is effective if most accesses are hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count example\n",
    "\n",
    "### Unsorted list of words\n",
    "\n",
    "<img src=\"./01.04h slide - unsorted = poor locality_small.png\">\n",
    "\n",
    "+ temporal locality for very common words like \"the\"\n",
    "+ unsorted list has no spatial locality - lots of jumping around to find words\n",
    "\n",
    "### Sort list of words / good locality\n",
    "\n",
    "<img src=\"./01.04i slide - sorted = good locality_small.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row vs. Column Major Layout\n",
    "\n",
    "The way you traverse a 2D array effects speed.\n",
    "\n",
    "+ numpy arrays are row-major order by default\n",
    "+ consider the following array:\n",
    "\n",
    "<img src=\"01.05b slide - row major_small.png\">\n",
    "\n",
    "+ in the real-world, column elements may be 1000's of locations apart\n",
    "+ scanning the array row by row is mor local than scanning column by column\n",
    "+ locality implies speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example provided by my instructor:\n",
    "\n",
    "<img src=\"01.05d slide - example row vs col scan speed diff_small.png\">\n",
    "\n",
    "I saw a smaller difference when I tied something similar below.  The lecture is a few years old, so maybe the numpy project has done some optimizations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2000)\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = 2000\n",
    "a_row = np.arange(n) / n\n",
    "a = np.arange(n)\n",
    "\n",
    "# make a big matrix\n",
    "for i in range(n-1):\n",
    "    a = np.vstack((a, a_row))\n",
    "    \n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0e+00, 1.0e+00, 2.0e+00, 3.0e+00, 4.0e+00, 5.0e+00, 6.0e+00,\n",
       "        7.0e+00, 8.0e+00, 9.0e+00],\n",
       "       [0.0e+00, 5.0e-04, 1.0e-03, 1.5e-03, 2.0e-03, 2.5e-03, 3.0e-03,\n",
       "        3.5e-03, 4.0e-03, 4.5e-03],\n",
       "       [0.0e+00, 5.0e-04, 1.0e-03, 1.5e-03, 2.0e-03, 2.5e-03, 3.0e-03,\n",
       "        3.5e-03, 4.0e-03, 4.5e-03],\n",
       "       [0.0e+00, 5.0e-04, 1.0e-03, 1.5e-03, 2.0e-03, 2.5e-03, 3.0e-03,\n",
       "        3.5e-03, 4.0e-03, 4.5e-03],\n",
       "       [0.0e+00, 5.0e-04, 1.0e-03, 1.5e-03, 2.0e-03, 2.5e-03, 3.0e-03,\n",
       "        3.5e-03, 4.0e-03, 4.5e-03],\n",
       "       [0.0e+00, 5.0e-04, 1.0e-03, 1.5e-03, 2.0e-03, 2.5e-03, 3.0e-03,\n",
       "        3.5e-03, 4.0e-03, 4.5e-03],\n",
       "       [0.0e+00, 5.0e-04, 1.0e-03, 1.5e-03, 2.0e-03, 2.5e-03, 3.0e-03,\n",
       "        3.5e-03, 4.0e-03, 4.5e-03],\n",
       "       [0.0e+00, 5.0e-04, 1.0e-03, 1.5e-03, 2.0e-03, 2.5e-03, 3.0e-03,\n",
       "        3.5e-03, 4.0e-03, 4.5e-03],\n",
       "       [0.0e+00, 5.0e-04, 1.0e-03, 1.5e-03, 2.0e-03, 2.5e-03, 3.0e-03,\n",
       "        3.5e-03, 4.0e-03, 4.5e-03],\n",
       "       [0.0e+00, 5.0e-04, 1.0e-03, 1.5e-03, 2.0e-03, 2.5e-03, 3.0e-03,\n",
       "        3.5e-03, 4.0e-03, 4.5e-03]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3997000.499999999\n",
      "Wall time: 881 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# scan column by column\n",
    "s=0\n",
    "for i in range(n): s += sum(a[:,i])\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3997000.5\n",
      "Wall time: 855 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# scan row by row\n",
    "s=0\n",
    "for i in range(n): s += sum(a[i,:])\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory vs SSD Experiments\n",
    "\n",
    "<img src=\"01.06a slide - latency experiments table of result_small.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Memory Heirarchy\n",
    "\n",
    "<img src=\"01.07b slide - the memory hierarchy_small.png\">\n",
    "\n",
    "+ Real system have several large storage types:\n",
    "  - Top of hierachy: small and fast storage close to CPU\n",
    "  - Bottom of hierarchy: large and slow storage further from CPU\n",
    "+ Caching is used to transfer data between different levels fo the hierarchy.\n",
    "+ To the programmer / compiler does not need to know\n",
    "  - The hardware provides an <font style=\"color: red; font-weight: bold\">abstraction</font>: memory looks like a single large array\n",
    "+ But performance depends on program's access pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Memory Locality in a Single Machine to Many Machines\n",
    "\n",
    "<img src=\"01.07c slide - clusters extend the memory hierarchy_small.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset (RDD) - Introduction\n",
    "\n",
    "+ The special thing about data processing clusters is that they share their memory.\n",
    "+ Locality in this context means that the data the CPU needs for its computation resides on the same computer as the CPU.\n",
    "+ Since this is not always the case, we need a mechanism for tranferring data between computers\n",
    "+ Preferably, this mechanism should be hidden from the developer.\n",
    "+ In Spark, this system abstraction is called a <font style=\"color: red; font-weight: bold\">resilient distibuted dataset, or RDD</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes and latencies in a typical memory hierarchy\n",
    "\n",
    "<img src=\"01.07d slide - sizes of latencies in a typical memory hierarchy_small.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making History: Google 2003\n",
    "\n",
    "+ Lary Page and Sergey Brin develop a method for storing very large files on multiple <font style=\"color: red; font-weight: bold\">commodity</font> computers.\n",
    "+ Each file is broken in fixed-sized **chuncks**.\n",
    "+ Each chunk is stored on <mark>multiple</mark> **chunk servers**.\n",
    "+ The location of the chunks is managed by the **master**\n",
    "\n",
    "<img src=\"01.08f slide - large files broken into chunks_small.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS: Distibuting Chunks\n",
    "\n",
    "<img src=\"01.08g slide - chunks are distributed randomly_small.png\" width=\"500\" height=\"600\">\n",
    "\n",
    "<img src=\"01.08h slide - chunks are distributed randomly_small.png\" width=\"500\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of GFS/HDFS\n",
    "\n",
    "+ <font style=\"color: red; font-weight: bold\">Commodity Hardware:</font> Low cost per byte of storage.\n",
    "+ <font style=\"color: red; font-weight: bold\">Locality:</font> data stored close to CPU.\n",
    "+ <font style=\"color: red; font-weight: bold\">Redundancy:</font> can recover from server failures.\n",
    "+ <font style=\"color: red; font-weight: bold\">Simple abstraction:</font> looks to user like standard file system (files, directories, etc.) Chunk mechanism is hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality\n",
    "\n",
    "+ tempting to process on just the top or middle server shown below\n",
    "+ better solution is to process the two chunks on two different servers\n",
    "  - Map-Reduce allows us to do this which we'll discuss shortly\n",
    "\n",
    "<img src=\"01.08k slide - locality with HDFS_small.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redundancy\n",
    "\n",
    "If the middle server goes down, we can recover without any loss of information.\n",
    "\n",
    "<img src=\"01.08j slide - redundancy with HDFS_small.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map-Reduce\n",
    "\n",
    "+ HDFS is a <font style=\"color: red; font-weight: bold\">storage</font> **abstraction**\n",
    "+ <font style=\"color: blue; font-weight: bold\">Map-Reduce</font> is a <font style=\"color: red; font-weight: bold\">computation</font> **abstraction** that works well ith HDFS\n",
    "+ Allows developers to specify parallel computation without knowing how the hardware is organized.\n",
    "+ Foundation for Hadoop and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two ways to do the same thing\n",
    "\n",
    "Here's a simple example of computing the squares of numbers in a list.  They give the same result, but the difference in **how** the result is computed is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "S=[]\n",
    "# usual way to compute squares\n",
    "for i in L:\n",
    "    S.append(i*i)\n",
    "    \n",
    "# or similarly, using list comprehension\n",
    "# S = [i * i for i in L]\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "S=[]\n",
    "# other way - map(fun, iter)\n",
    "S = map(lambda x: x*x, L)\n",
    "print(list(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Unnecessary Constraint\n",
    "\n",
    "Doing this simple operation in the **usual way** is telling the computer what order to do the computation.  But this order constraint is not necessary.  By specifying the computation the **other way**, we give the interpreter an opportunity to take advantage of parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a feel for Map-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# sum of squares the usual way\n",
    "\n",
    "sum_squares = sum([i*i for i in L])\n",
    "print(sum_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# sum of square as an execution plan\n",
    "\n",
    "import functools as ft\n",
    "# ft.reduce() takes in a function and a list as an argument.\n",
    "# The function is called with a lambda function and an iterable and a new reduced result is returned\n",
    "#     function ---------------------v,              v------- generates list of square as shown above\n",
    "sum_squares_ex_plan = ft.reduce((lambda x,y: x+y), map(lambda i: i*i, L))\n",
    "print(sum_squares_ex_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Why is this a wrong way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THE WRONG WRONG WRONG WAY!!!\n",
    "\n",
    "sum_squares_wrong_way = ft.reduce(lambda x,y: x + y*y, L)\n",
    "print(sum_squares_wrong_way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Order Independence?\n",
    "\n",
    "To compute things in parallel, we have to organize our computation such that order doesn't matter.  For example, look at a sum:\n",
    "\n",
    "<img src=\"./Order independence, sum example.png\">\n",
    "\n",
    "+ Computation order can be chosen by compiler/interpreter/optimizer\n",
    "+ Allows for **parallel computation** on subsets of data\n",
    "  - Modern hardware calls for parallel computation, but parallel computation is very hard to program\n",
    "+ Using map-reduce, developer **exposes** to the compiler opportunities for parallel computation.\n",
    "  - telling the compiler:  _\"I don't care what order you do the sum (or whatever calculation). I just want the correct final result.\"_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Computing Evolution\n",
    "\n",
    "<img src=\"./Google FS 2003.png\" width=\"700\" height=\"850\">\n",
    "\n",
    "+ Master knows where all the data is\n",
    "+ data itself is chopped up into chunks\n",
    "+ and distibuted across chunk servers as described earlier\n",
    "\n",
    "## Google Map-Reduce, 2004\n",
    "\n",
    "<img src=\"01.10b slide - Google Map-Reduce 2004_small.png\" width=\"700\" height=\"850\">\n",
    "\n",
    "## Apache Hadoop, 2006\n",
    "\n",
    "<img src=\"01.10c slide - Apache Hadoop 2006_small.png\">\n",
    "\n",
    "## Apache Spark, 2014\n",
    "\n",
    "+ Matei Zaharia, MPLab, Berkeley (Now in MIT)\n",
    "+ Main difference from Hadoop: maximizes memory uses instead of just using disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark, java, scala & python\n",
    "\n",
    "+ The native language of the Hadoop eco-system (predecessor to Spark) is Java\n",
    "+ Spark can  be programmed in Java, but code tends to be long\n",
    "+ **Scala** (another JVM-based language) allows parallel programming to be abstracted. It's the core language of Spark\n",
    "  - The main problem with Scala is that is has a small user base.\n",
    "  - Need to learn Scala if you want to extend Spark.\n",
    "+ **PySpark** provides the Python API for Spark programming\n",
    "  - Does not always achieve the same efficiencies but is easier to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Context\n",
    "\n",
    "+ The pyspark program runs on the main node\n",
    "+ cont w/ 01.10f content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distibuted Dataset (RDD) - Main Spark Data Structure\n",
    "\n",
    "+ A list whose elements are distibuted over several computers\n",
    "+ When in RDD form, the elements of the list can be manipulated only through RDD specific methods\n",
    "  - Why we need to do this will become clear shortly\n",
    "+ RDDs can be created from a list on the master node or from a file.\n",
    "+ RDDs can be translated back to a local list using `collect()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Examples\n",
    "\n",
    "Sum of squares:  \n",
    "<img src=\"01.10h slide - basic sum of squares example_small.png\">\n",
    "\n",
    "Squares of a list:  \n",
    "<img src=\"01.10j slide - RDD to RDD transform example_small.png\">\n",
    "\n",
    "Both of these examples **materialize** the results.  More on materialization shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheap Checks\n",
    "\n",
    "As we develop Spark, we need to adopt a _computationally-frugal_ mindset.  Instead of displaying the entire result, consider displaying the first or the first `n` items or even taking a sample using:\n",
    "\n",
    "+ `RDD.first()`\n",
    "+ `RDD.take(n)`\n",
    "+ `RDD.sample(withReplacement, fraction, seed=None)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Organization\n",
    "\n",
    "<img src=\"0111a_hardware_organization.png\">\n",
    "\n",
    "### In local installation, cores serve as master & slaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization\n",
    "\n",
    "<img src=\"0111b_spatial_software_organization.png\">\n",
    "\n",
    "+ Each RDD is partitioned among the workers\n",
    "+ Workers manager **partitions** and **executors**\n",
    "+ Executors execute **tasks** on their partitions, are myopic (only focused on their chunk of processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Organization (more detail)\n",
    "\n",
    "<img src=\"0111d_spatial_organization_details_small.png\">\n",
    "\n",
    "+ SparkContext (sc) is the abtractionthat encapsulates the cluster for the driver node (and the developer).\n",
    "+ Worker nodes manage resources in a single slave machine.\n",
    "+ Worker nodes communicate with the cluster manager.\n",
    "+ Executors are the processes that can perform **tasks**.\n",
    "+ Cache refers to the local memory on the slave machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materialization \n",
    "\n",
    "+ An RDD is essentially an execution plan until a result needs to be stored in memory.\n",
    "+ Results that need to strored in memory are said to be **materialized**.\n",
    "\n",
    "## RDD Processing\n",
    "\n",
    "<img src=\"01.11f slide - RDD Processing_small.png\">\n",
    "\n",
    "+ RDDs by default, are not materialzized\n",
    "+ They do materialize if cached or otherwise persisted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temportal Organization - RDD Graph and Physical Plan\n",
    "\n",
    "<img src=\"01.11h slide - Temporal vs Spatial organization_small.png\">\n",
    "\n",
    "## Sumary of Terms and concepts of execution - _keep the earlier pictures in mind_\n",
    "\n",
    "+ RDDs are **partitioned** across workers.\n",
    "+ RDD graph defines the **lineage** of the RDDs.\n",
    "+ SparkContext divides the RDD graph into **stages** which defines the execution plan (corresponding to physical plan)\n",
    "+ A **task** corresponds to _one stage_ and is restricted to _one partitions_.\n",
    "+ An **executor** is a process that performs tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three groups of Spark commands\n",
    "\n",
    "+ **Creation:** RDD from files, databases, or data on driver node.\n",
    "+ **Transformations:** RDD to RDD (no materialization)\n",
    "+ **Actions:** RDD to data on drive node, databases, files (causes materialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _\"Nobody works with RDDs\"_\n",
    "\n",
    "+ Spark Dataframe = RDD + schema (meta-data)\n",
    "+ Spark SQL = query large Spark data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whale Classification\n",
    "\n",
    "+ Problem Description and EDA notebook\n",
    "+ Classification notebook\n",
    "+ Boosted Trees vs. Random Forest\n",
    "  - which is more suited for Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function Logic\n",
    "\n",
    "### Correction to image below: subscript on test features should go to 12 (not 10)\n",
    "\n",
    "### Predictions are made using the following procedure:\n",
    "\n",
    "1. generate boostrap predictions (columns on right of the image)\n",
    "2. filter out the values below the 10th percentile and valuse above 90th percentile (minR=0.10, maxR=0.90)\n",
    "3. use logic defined in the **Calculating predictions** section of the notebook:\n",
    "  - If respective minimum score and maximum score values are both less than 0, predict -1 (Cuvier's)\n",
    "  - If respective minimum score value is less than 0 and maximum score value is greater than 0, predict 0 (Unsure)\n",
    "  - If respective minimum score and maximum score values are both greater than 0, predict 1 (Gervais)\n",
    "\n",
    "<img src=\"./whale_classification_logic.jpg\">\n",
    "\n",
    "After accumulating the bootstrap margin scores:\n",
    "\n",
    "+ sort each row low to high from left to right: line 35 of cell 14\n",
    "+ filter out the lower 10% (minR=0.1) and upper 90% (maxR=0.9): line 36 of cell 14\n",
    "+ predict on the test set based on criteria described in the **Calculating predictions** and **Computing the score ranges** sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "+ http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=rdd#pyspark.RDD \n",
    "+ tbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to discussion questions\n",
    "\n",
    "Q1. Map, Reduce should not depend on: order of items in the list (commutativity) or order of operations (associativity).  It is this independence that allows parallel computation.\n",
    "\n",
    "Q2. tbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./00a_hadoop_vs_spark.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
